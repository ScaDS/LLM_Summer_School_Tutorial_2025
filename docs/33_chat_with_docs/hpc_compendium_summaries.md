* accessibility.md:
This document is an accessibility statement for the Technische Universität Dresden's websites, outlining the university's efforts to make its online presence accessible in accordance with German laws and regulations, and providing contact information for reporting accessibility issues and seeking remedies.

* data_protection_declaration.md:
This document outlines a data protection policy, stating that only IP addresses are collected for error analysis and stored temporarily, with users having the right to request information about their data and contact relevant authorities if needed.

* index.md:
This document provides an overview of the High-Performance Computing (HPC) systems and services offered by the Center for Information Services and High-Performance Computing (ZIH) at TU Dresden, including documentation, contribution guidelines, news, training courses, and support options.

* legal_notice.md:
This document provides a legal notice for a website or repository affiliated with the Technical University of Dresden, outlining the responsible parties, contact information, and licensing terms for documentation and software components.

* access/desktop_cloud_visualization.md:
NICE DCV is a remote access solution that enables users to run OpenGL 3D applications on ZIH systems using the server's GPUs, with optional access through JupyterHub and instructions for checking GPU support and configuring the environment.

* access/graphical_applications_with_webvnc.md:
This document describes how to run graphical applications on ZIH systems using WebVNC, a solution based on a Singularity container with a VNC setup that can be accessed via JupyterHub or a terminal, providing a web-based client that eliminates the need for additional software.

* access/jupyterhub.md:
The ZIH (Zentrum für Informationsdienste und Hochleistungsrechnen) offers a JupyterHub service, providing an easy-to-use environment for working with Jupyter notebooks on their systems, with features such as customizable profiles, JupyterLab, and support for various programming languages like Python, R, and Julia.

* access/jupyterhub_custom_environments.md:
This document provides a step-by-step guide on creating and using custom environments for JupyterHub, allowing users to install their preferred Python packages and use them in their notebooks on various architectures at ZIH systems.

* access/jupyterhub_for_teaching.md:
This document introduces useful features of JupyterHub for teaching, including cloning repositories, spawning options, creating shared Python environments, and setting up custom Jupyter kernels, to help educators effectively utilize the platform for their courses.

* access/jupyterhub_teaching_example.md:
This document provides a step-by-step guide on setting up a Jupyter Lab course using JupyterHub, including creating custom environments, cloning repositories, and activating environments for students to work on notebooks during a course.

* access/jupyterlab.md:
This document provides step-by-step instructions on how to access JupyterLab without using JupyterHub on various clusters, including Alpha Centauri, Barnard, Capella, Romeo, and Visualization, using either port forwarding or X11 forwarding methods.

* access/jupyterlab_user.md:
Users can now install a custom version of JupyterLab, although support will not be provided by the technicians, and a step-by-step installation process is outlined in the document.

* access/key_fingerprints.md:
This document provides a list of up-to-date key fingerprints for various login nodes and dataport nodes of high-performance computing (HPC) systems, to help users verify the authenticity of the servers they are connecting to.

* access/overview.md:
The ZIH systems can be accessed through various methods, including SSH, Desktop Cloud Visualization, WebVNC, and JupyterHub, but require a HPC project, login, and connection via the university's network or a Virtual Private Network (VPN) for security reasons.

* access/security_restrictions.md:
Following a security incident, the German HPC sites in Gauß Alliance, including ZIH systems, have implemented new security restrictions, such as mandatory password changes, updated SSH key requirements, and limited access to and from the systems, to prevent malware infection and spreading.

* access/ssh_login.md:
This document provides a step-by-step guide on how to connect to the ZIH systems via terminal on Linux, Mac, and Windows operating systems, including setting up SSH keys, configuring default parameters, and enabling X11-forwarding for graphical applications.

* access/ssh_mobaxterm.md:
This markdown document provides a step-by-step guide on how to connect to a remote server using MobaXterm, a terminal emulator for Windows, including download and installation, configuration of local settings, and starting a new SSH session.

* access/ssh_putty.md:
This document provides a step-by-step guide on how to connect to a remote server using PuTTY, a free and open-source terminal emulator, on a Windows system, including downloading and installing PuTTY, starting a new SSH session, and configuring connection details such as username, SSH-key, and X-forwarding.

* application/acknowledgement.md:
The document requests that users of the NHR center at TU Dresden's high-performance computing (HPC) systems acknowledge the resource usage in their publications, providing examples of acknowledgement statements in different scenarios.

* application/overview.md:
This document provides information on the application process for using High-Performance Computing (HPC) resources at the TU Dresden, including eligibility criteria, project types, and the application workflow through the JARDS system.

* application/project_management.md:
This document provides an overview of the user management responsibilities and tools available to project leaders and administrators for managing High-Performance Computing (HPC) projects at the Technical University of Dresden, including adding and removing users, monitoring resources, and tracking project statistics.

* application/terms_of_use.md:
This document outlines the Terms of Use for the High-Performance Computing (HPC) resources at the Center for Information Services and High-Performance Computing (ZIH) at TU Dresden, highlighting key points such as data storage, project responsibilities, and user obligations, with only the German version being legally binding.

* contrib/content_rules.md:
This markdown document outlines the content rules and guidelines for contributing to a documentation project, covering aspects such as writing style, Markdown syntax, code blocks, spelling, and technical wording, to ensure consistency and quality in the documentation.

* contrib/contribute_browser.md:
This document provides a step-by-step guide on how to contribute to the HPC documentation of TU Dresden/ZIH using GitLab's web interface, including preparation, creating a branch, editing and adding articles, submitting for publication, and undergoing the revision process.

* contrib/contribute_container.md:
This markdown document provides a step-by-step guide on how to contribute to the HPC documentation of TU Dresden/ZIH via a local clone of the Git repository, including setup, working with the local clone, merging forked repositories, and using tools to ensure quality, such as Docker containers and automated checks for links, spelling, and text format.

* contrib/howto_contribute.md:
This documentation provides guidelines on how to contribute to the HPC Compendium, including technical setup, content rules, Git workflow, and various ways to contribute, such as via issue, web IDE, or local clone, to help maintain a high-quality and consistent documentation.

* data_lifecycle/data_sharing.md:
This document provides guidance on sharing data with other users or projects, including commands for granting access to files or directories, managing access rights using Access Control Lists (ACLs), and setting permissions for groups and individual users.

* data_lifecycle/file_systems.md:
This document provides an overview of the different filesystems available on ZIH systems, including permanent and working filesystems, and offers recommendations for efficient usage, troubleshooting, and debugging of filesystem issues.

* data_lifecycle/longterm_preservation.md:
This markdown document provides guidance on the long-term preservation of research data, including why and what data should be preserved, the importance of adding meta-data, and options for archiving data at the ZIH (Center for Information Services and High Performance Computing) of TU Dresden.

* data_lifecycle/lustre.md:
This document provides guidelines and useful commands for optimizing performance and managing storage on a Lustre filesystem, a high-performance parallel file system often used in high-performance computing environments.

* data_lifecycle/overview.md:
This document provides guidelines for efficient data life cycle management in high-performance computing (HPC) projects, covering topics such as data storage, backup, folder structure, metadata, data hygiene, and access rights to ensure consistency, collaboration, and security throughout the project.

* data_lifecycle/permanent.md:
This document provides an overview of the permanent filesystems available on a high-performance computing (HPC) system, including their usage guidelines, quotas, and backup policies to help users manage their data efficiently.

* data_lifecycle/working.md:
This document provides an overview of the various filesystems available on ZIH systems, including their capacities, performances, and usage recommendations, as well as tips for efficient usage and debugging common filesystem issues.

* data_lifecycle/workspaces.md:
The document describes the concept of "Workspaces" at ZIH, which is a mechanism to manage data on high-performance computing (HPC) systems, allowing users to allocate temporary storage spaces with limited lifetimes and quotas, and providing tools to list, extend, restore, and delete these workspaces, as well as features for cooperative usage and troubleshooting.

* data_transfer/datamover.md:
The Datamover is a specialized data transfer system within the ZIH HPC systems that provides high-speed file transfers between different filesystems using unique commands such as `dtcp`, `dtmv`, and `dtrsync`, while also allowing management of transfer jobs and integration with external systems like group drives.

* data_transfer/dataport_nodes.md:
This document provides guidance on transferring large data to and from ZIH systems using dataport nodes, which offer higher bandwidth and are designed for this purpose, and explains how to use tools like SCP, SFTP, and Rsync to transfer files from Linux and Windows systems.

* data_transfer/object_storage.md:
This document provides instructions on how to transfer data between ZIH systems and object storage (S3) using the `rclone` module, including initial configuration, copying data to and from object storage, and accessing stored files from various locations.

* data_transfer/overview.md:
This document provides guidance on data transfer to and from ZIH systems using tools like `scp`, `rsync`, and `sftp` via dataport nodes, as well as data transfer inside ZIH systems using the Datamover service.

* jobs_and_resources/alpha_centauri.md:
The Alpha Centauri GPU cluster is a multi-GPU system installed for AI-related computations, featuring 48 physical cores, 8 NVIDIA A100-SXM4 GPUs, and 1 TB RAM per node, with various software and usage options available, including modules, Python virtual environments, JupyterHub, and Singularity containers.

* jobs_and_resources/arm_hpc_devkit.md:
The NVIDIA Arm HPC Developer Kit is a system provided by the ZIH for experimentation with Arm-based systems, featuring an Ampere Altra Q80-30 processor, 512G DDR4 memory, and 2x NVIDIA A100 GPUs, and can be accessed via SSH after obtaining permission from the hpcsupport team.

* jobs_and_resources/binding_and_distribution_of_tasks.md:
This document provides an overview of task binding and distribution strategies in Slurm, a workload manager, including various options for binding tasks to cores, sockets, and nodes, and their potential impact on application execution time.

* jobs_and_resources/capella.md:
The Lenovo multi-GPU cluster "Capella" is a high-performance computing system installed for AI-related computations and traditional HPC simulations, fully integrated into the ZIH HPC infrastructure and ranked #51 in the TOP500 list of the world's fastest computers.

* jobs_and_resources/checkpoint_restart.md:
This document discusses the concept of checkpointing and restarting in high-performance computing (HPC) systems, which allows users to save the state of a running process and resume it later, preventing losses due to system failures or timeouts, and providing an example of how to use the Distributed Multi-Threaded Check-Pointing (DMTCP) tool to implement checkpointing in Slurm batch jobs.

* jobs_and_resources/hardware_overview.md:
The ZIH systems offer a range of High Performance Computing (HPC) resources, including six homogeneous clusters with over 100,000 CPU cores and a peak performance of more than 1.5 quadrillion floating point operations per second, tailored to support data-intensive computing, Big Data analytics, and artificial intelligence methods.

* jobs_and_resources/julia.md:
The SMP Cluster Julia is a high-performance computing system featuring a large shared memory node, optimized for data-intensive applications, with 370 TB of NVMe storage and configurable quotas for projects, suitable for OpenMP and Open MPI applications with large memory demands.

* jobs_and_resources/mpi_issues.md:
This document outlines several known issues with MPI (Message Passing Interface) implementations, specifically with Open MPI, including performance losses, incorrect resource distribution, and segmentation faults, along with suggested workarounds and solutions for each problem.

* jobs_and_resources/nvme_storage.md:
The document describes the configuration of 90 NVMe storage nodes, each equipped with Intel NVMe SSDs, InfiniBand links, Intel Xeon processors, and RAM, capable of high-speed data transfer.

* jobs_and_resources/overview.md:
This markdown document provides an introduction to the High Performance Computing (HPC) resources and jobs at the ZIH, including the available hardware, selection of suitable clusters, job submission, and resource management, to help users effectively utilize the HPC systems for their research goals.

* jobs_and_resources/power9.md:
The GPU Cluster Power9 is a multi-GPU cluster that was installed in 2018 and has been re-engineered as a standalone cluster with its own Slurm batch system and login nodes, featuring six NVIDIA Tesla V100 GPUs per node and supporting AI, analytics, and data-intensive workloads.

* jobs_and_resources/romeo.md:
The CPU Cluster Romeo is a general-purpose cluster based on AMD Rome CPUs, consisting of a homogeneous standalone system with its own Slurm batch system and login nodes, offering 128 physical cores and 512 GB of main memory per node, with specific usage guidelines and considerations for running jobs and utilizing software modules.

* jobs_and_resources/slurm.md:
Here is a one-sentence summary of the markdown document in English:

This document provides a comprehensive overview of the batch system Slurm used for resource management and job scheduling at ZIH, including how to submit jobs, manage resources, and use various Slurm options and commands to control and monitor jobs.

* jobs_and_resources/slurm_examples.md:
This markdown document provides examples and guidelines for submitting various types of jobs, including parallel jobs, MPI jobs, and array jobs, as well as requesting exclusive resources, GPUs, and chain jobs, using the Slurm job scheduler on a high-performance computing cluster.

* jobs_and_resources/slurm_examples_with_gpu.md:
This markdown document provides guidance on requesting and utilizing GPU resources via the Slurm batch system, including examples of job scripts for requesting GPUs, limitations on GPU job allocations, and running multiple GPU applications simultaneously in a batch job.

* jobs_and_resources/slurm_generator.md:
This markdown document describes a Slurm Job File Generator, a tool that helps users create job files for submitting high-performance computing (HPC) jobs to a Slurm-managed cluster.

* jobs_and_resources/slurm_limits.md:
This document outlines the resource limits for using the Slurm batch system on ZIH systems, including restrictions on runtime and memory usage, to ensure efficient and fair usage of shared computing resources.

* quickstart/getting_started.md:
This markdown document provides a comprehensive guide for new users to get started with the High Performance Computing (HPC) systems at the ZIH, including applying for a login, accessing the systems, transferring code and data, accessing software, and running parallel HPC jobs.

* software/big_data_frameworks.md:
This document provides an overview of using Big Data analytics frameworks, including Apache Spark, Apache Flink, and Apache Hadoop, on the ZIH systems, covering prerequisites, usage in interactive and batch jobs, and with Jupyter notebooks, as well as troubleshooting tips.

* software/building_software.md:
To build software on the system, it's recommended to use a job and a separate build directory in the `/data/horse` filesystem, rather than compiling directly in the read-only `/projects` filesystem, and then install the software back to `/projects` using the login nodes.

* software/cfd.md:
This markdown document provides an overview of the Computational Fluid Dynamics (CFD) applications available on a system, including OpenFOAM, Ansys CFX, Ansys Fluent, ICEM CFD, and STAR-CCM+, along with examples of job scripts and usage instructions for each application.

* software/cicd.md:
The document describes how to set up a Continuous Integration/Continuous Deployment (CI/CD) pipeline using GitLab Runner on the ZIH systems, allowing users to continuously build, test, and benchmark their HPC software in the target environment.

* software/compilers.md:
This document provides an overview of the compilers and flags available on the ZIH system, including GNU, Clang, Intel, and PGI compilers, and explains how to use various compiler flags to optimize code for performance, debugging, and specific architectures.

* software/containers.md:
This markdown document describes the use of Singularity, a containerization platform, on ZIH systems, including its installation, creation of custom containers, and usage, allowing users to run applications in a controlled environment with full control over dependencies and libraries.

* software/custom_easy_build_environment.md:
This markdown document provides a step-by-step guide on how to use EasyBuild to install custom software modules on the ZIH systems, including setting up a workspace, allocating nodes, loading the correct module environment, and building and loading custom modules using EasyBuild.

* software/data_analytics.md:
This document provides an overview of the data analytics tools and resources available on ZIH systems, including Python, R, and Big Data frameworks, as well as guidance on how to access and utilize these tools, manage data, and set up collaborative workflows.

* software/data_analytics_with_python.md:
This markdown document provides an introduction to using Python for data analytics on the ZIH system, covering topics such as Python consoles, virtual environments, Jupyter notebooks, parallel computing with Pandas and Dask, and using MPI for Python with mpi4py.

* software/data_analytics_with_r.md:
This document provides an overview of using R for data analytics, including its programming language and environment for statistical computing and graphics, and how to use it on a cluster with Slurm job scheduler, including interactive and batch jobs, RStudio, installing packages, deep learning with TensorFlow, and parallel computing methods such as shared-memory and distributed-memory parallelism using libraries like parallel, Rmpi, and foreach.

* software/data_analytics_with_rstudio.md:
This document provides an overview of using RStudio, an integrated development environment for R, on ZIH systems, including the option to run it directly in the browser through JupyterHub.

* software/debuggers.md:
This markdown document provides an overview of debugging tools and techniques available at ZIH systems, including an introduction to parallel debugging, advice on how to prepare code for debugging, and detailed information on using debuggers such as GDB and Arm DDT, as well as memory debugging tools like Valgrind.

* software/distributed_training.md:
This document provides an overview of distributed training in machine learning, covering concepts such as data parallelism and model parallelism, and offering guidance on implementing distributed training using frameworks like TensorFlow, PyTorch, and Horovod, with examples and code snippets to facilitate the process.

* software/energy_measurement.md:
The Intel Haswell nodes of the ZIH system are equipped with power instrumentation that allows for the recording and accounting of power dissipation and energy consumption data, which can be accessed through various interfaces, including command line tools, Slurm tools, and a C API.

* software/fem_software.md:
This markdown document provides an overview of various finite element method (FEM) software packages available on ZIH systems, including Abaqus, Ansys, COMSOL Multiphysics, and LS-DYNA, with detailed instructions on how to use them interactively and in batch mode, along with examples and tips for efficient usage.

* software/gpu_programming.md:
This markdown document provides a comprehensive guide to GPU programming, including available GPUs, using GPUs with Slurm, directive-based GPU programming with OpenACC and OpenMP, native GPU programming with CUDA, and performance analysis tools such as NVIDIA nvprof, Visual Profiler, Nsight Systems, and Nsight Compute.

* software/hyperparameter_optimization.md:
Here is a one-sentence summary of the markdown document in English:

The OmniOpt tool performs hyperparameter optimization for classical simulations and machine learning algorithms, and this document provides a step-by-step guide on how to use OmniOpt, including preparation of the application script, configuration, and evaluation of results.

* software/licenses.md:
This document explains how users can install and use their own software licenses on ZIH systems, including configuring license servers and environment variables to override default settings.

* software/lo2s.md:
The `lo2s` tool is a lightweight, node-level performance monitoring tool that creates parallel OTF2 traces, providing a detailed view of both application and system performance, and can operate in either process monitoring or system monitoring mode.

* software/machine_learning.md:
This markdown document provides an introduction to running machine learning applications on ZIH systems, covering topics such as recommended GPU clusters, loading modules, using Python and R for machine learning, working with Jupyter Notebooks and containers, and accessing additional libraries and datasets for machine learning tasks.

* software/mathematics.md:
This markdown document provides an overview of mathematics applications, including the use of Mathematica and MATLAB on a high-performance computing system, with instructions on how to install, configure, and utilize these tools for various mathematical tasks, including parallel computing and batch jobs.

* software/math_libraries.md:
This document provides an overview of various high-quality mathematics libraries available for use, including BLAS, LAPACK, ScaLAPACK, AOCL, and MKL, which offer efficient and optimized implementations of linear algebra and other mathematical functions for different hardware architectures.

* software/modules.md:
The document describes the environment modules system used to manage software on high-performance computing (HPC) systems, providing a way to dynamically modify user environments and access various compilers, libraries, and utilities.

* software/mpi_usage_error_detection.md:
The ZIH maintains and supports the MUST tool to automatically check if MPI applications conform to the MPI standard and to detect errors or non-portable constructs, and this document provides a guide on how to set up and use MUST to check the correctness of MPI applications.

* software/nanoscale_simulations.md:
This markdown document provides an overview of various software packages available for nanoscale simulations, including ABINIT, CP2K, CPMD, GAMESS, Gaussian, GROMACS, LAMMPS, NAMD, ORCA, Siesta, and VASP, along with instructions on how to access and use these packages on a high-performance computing system.

* software/ngc_containers.md:
This markdown document provides a guide on using GPU-accelerated containers for deep learning, specifically NVIDIA's NGC containers, on the ZIH system, including how to run them on single and multiple GPUs, as well as the limitations and potential issues with multi-node usage.

* software/overview.md:
This document provides an overview of the environment and software used on ZIH systems, covering the user environment, software environment, and various options for working with software, including modules, Jupyter Notebook, and containers.

* software/papi.md:
This document provides an introduction to the Performance Application Programming Interface (PAPI), a tool for measuring CPU performance counters, and guides users on how to use PAPI's high-level and low-level APIs to collect performance events on various computer systems, including those at the ZIH (Zentrum für Informationsdienste und Hochleistungsrechnen).

* software/performance_engineering_overview.md:
This document provides an overview of performance engineering, a process aimed at ensuring that computing systems meet expectations for performance, including techniques, objectives, and tools used to measure, analyze, and optimize system performance.

* software/perf_tools.md:
This document provides an overview of the Linux "perf" command, a performance analysis tool that provides detailed information about system and application performance, including sampling, tracing, and counting of various hardware and software events.

* software/pika.md:
This document describes PIKA, a hardware performance monitoring stack used to track and analyze the efficiency of Slurm jobs on high-performance computing (HPC) systems, providing a web interface for visualization and analysis of job performance data.

* software/power_ai.md:
This document provides an overview of the PowerAI framework for machine learning, including links to various documentation sources, user guides, and information on supported software packages, all specific to PowerAI version 1.5.4 and compatible with the Power9 cluster.

* software/private_modules.md:
This document provides a step-by-step guide on setting up and using private modules, which allow users to load their own installed software packages into their environment and manage different versions without conflicts, for both single users and project groups.

* software/python_virtual_environments.md:
This document provides guidance on using Python virtual environments on ZIH systems, covering the creation, activation, and management of virtual environments using `virtualenv` and `conda`, as well as best practices for persisting and recreating these environments.

* software/pytorch.md:
This document provides an overview of using PyTorch, an open-source machine learning framework, on a high-performance computing cluster, including installation, usage, and troubleshooting tips for working with PyTorch on various clusters and environments.

* software/scorep.md:
The Score-P measurement infrastructure is a scalable tool suite for profiling, event tracing, and online analysis of HPC applications, supporting various parallelization types and features, and can be easily used by prepending the Score-P wrapper to compile and link commands.

* software/singularity_power9.md:
This markdown document provides guidance on building Singularity containers for the Power9 architecture on ZIH systems, including the use of a virtual machine to gain root permissions, and introduces two helper programs, `buildSingularityImage` and `startInVM`, to simplify the process.

* software/singularity_recipe_hints.md:
This markdown document provides a collection of recipes and hints for using Singularity, a container platform, including example definitions for creating containers, running GUI applications, using hardware acceleration, and configuring temporary and cache directories.

* software/software_development_overview.md:
This markdown document provides an overview of software development and tools on the ZIH systems, covering topics such as compiling code, using libraries, debugging, and optimizing performance, along with helpful hints and questions to consider for ensuring reliable and efficient code.

* software/spec.md:
This document provides a guide on how to use the SPEChpc 2021 benchmark suite to compare the performance of different high-performance computing (HPC) systems and evaluate parallel strategies for applications on a target HPC system.

* software/tensorboard.md:
This document provides instructions on how to use TensorBoard, a visualization toolkit for TensorFlow, to inspect model training, either through JupyterHub or by using the TensorBoard extension of the TensorFlow module on ZIH systems.

* software/tensorflow.md:
This document provides an overview of using Neural Networks with TensorFlow on a cluster, including how to load TensorFlow modules, work with TensorFlow in various environments such as consoles, JupyterHub, and containers, and ensure compatibility between TensorFlow 1 and 2.

* software/utilities.md:
This markdown document provides an overview of various utilities and tools available on the ZIH systems to make life easier for computer scientists, including Tmux, a terminal multiplexer, lstopo for displaying system topology, and tools for working with large archives and compressed files, such as rapidgzip and Ratarmount.

* software/vampir.md:
This markdown document provides a comprehensive guide on using Vampir, a graphical analysis framework for studying the course of events in parallel programs, including its installation, usage, and advanced features such as VampirServer for scalable analysis capabilities.

* software/virtual_desktops.md:
This document provides instructions on how to use WebVNC or DCV to run GUI applications on high-performance computing (HPC) resources, including launching a virtual desktop, reconnecting to a session, and terminating a remote session.

* software/virtual_machines.md:
This markdown document provides instructions for users to build their own Singularity containers on ZIH systems using virtual machines (VMs), including how to create and access a VM, example usage, automation tools, and troubleshooting known issues.

* software/visualization.md:
This document provides an overview of using ParaView, an open-source data analysis and visualization application, on a high-performance computing cluster, including instructions for interactive and batch modes, as well as tips for optimizing performance and troubleshooting common issues.

* software/zsh.md:
This document introduces ZSH (z-shell) as an alternative shell to Bash, offering various convenience features, and provides examples and installation instructions for using ZSH with the oh-my-zsh plugin on Linux systems, particularly on ZIH systems.

* support/support.md:
This document outlines the user support options for the HPC system at TU Dresden, including creating a ticket by emailing hpc-support@tu-dresden.de and attending open Q&A sessions, and encourages users to first consult the documentation for answers to their questions.

